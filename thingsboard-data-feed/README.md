# Thingsboard Data Feed
This repo contains a script to read the contents of 5 provided files, each containing data generated by NASA C-MAPSS data simulator for jet engines. One data file, "anomalous.csv", contains anomalous data in a run-to-failure scenario. All other files, "non-anomalous*.csv", contain jet engine data without anomalies. 
Devices under device profile "Jet Engine 2" correspond with the anomalous dataset.

**Usage:**
python3 csv_parser_large.py [options]

	Options:
	-r, --realtime
	Sends 1 row from each dataset once a second. Without this flag, the script will send as much data as reasonably fast as possible to respective thingsboard devices.

**Notes:**
* This is a multi-processed script. It spawns 1 concurrent process per "jet engine" dataset, representing a single jet engine asset sending data independently of other "jet engines". Each process is responsible for its own set of 14 devices per jet engine, sending data over 14 MQTT clients (70 total clients) to the respective devices on thingsboard, accessed with tokens listed under "devices_J*".

* Assumes thingsboard is listening on port 30864 at host: "tb-route-mttq-transport-thingsboard.apps.thingsboard.openshift.solutions". To change this, edit these variables:
	* broker_port
	* broker_address
* If this script is needed to connect to a separate thingsboard instance, all device tokens will need to be changed as well to match new devices, respectively.

* This script must be run in an environment / subnet capable of reaching the thingsboard cluster running on a private subnet.

* For the sake of replicability, I've also included exports of all thingsboard device profiles, asset profiles, and rule chain used in this configuration under the folder "thingsboard_exports"
	* Unfortunately, individual assets and devices cannot be exported so those would need to be re-constructed if using a fresh thingsboard install.
